# pyvrp_docker ðŸšš

A Dockerized setup for running [PyVRP](https://github.com/PyVRP/PyVRP) â€” a high-performance solver for Vehicle Routing Problems â€” combined with a multiprocessing system for efficiently launching and managing large-scale experiments.

---

## ðŸ” What is PyVRP?

[PyVRP](https://github.com/PyVRP/PyVRP) is a hybrid C++/Python library implementing state-of-the-art heuristics for different VRP types:

This container allows you to run PyVRP CVRPs with your own distance matrices in parallel with preconfigured demand scenarios based on the X-Sets (https://www.sciencedirect.com/science/article/abs/pii/S0377221716306270) 

Use the debug.vrp file in data/Vrp-Set-X/X/debug.vrp for custom demand scenarios
---

### âš™ï¸Prerequisites

- pip/python
- pip install -r requirements.txt
- gcloud cli for remote deployment
- Custom distance matrices in json format like data/distance_matrices/Chicago_100x100_RoadData.json
- Docker Desktop/Docker cli

### Local Deployment
- The flask_endpoint.py launches a webserver with exposed ports --> Not recommended on local machine
- Can be adapted in the build_upload.sh and flask_endpoint.py for different behavior


### Build and Launch (Linux)

Normal GCP c4a-highcpu-8 launch (any c4a instance will do)

Requires: gcloud-cli with running vm instance (can be started in 4core/8core version using 4coreStart.sh or 8coreStart.sh)

```bash
# Clone the repository
git clone https://github.com/zrtgzrtg/pyvrp_docker.git
cd pyvrp_docker

# Build Docker image
chmod +x build_upload.sh
./build_upload.sh

# SSH into GCP VM
gcloud compute ssh chosen-VM-Name --zone=chosen-VM-zone
# in SSH session to enter filesystem of docker container
docker exec -it containerID /bin/bash
```
```bash
# During SSH 
# Copy out of docker container
docker cp containerID:/app/BatchCustom pathToCopyLoc(or BatchDir)
# Copy out of GCP
gcloud compute scp --recurse vmName:~/pathToDir pathToCopyLoc
```

# Logs

- logs for each process in runLogs/solver_{ID}.log
- batchprogress.log or progress.log as main log files

# Importing distance matrices

- Put distance matrices into data/distance_matrices and add entry into data/city_matrices.py
- This expects a pair of distance_matrices as the main objective of this project is the comparison between euclidean and real road network routing decisions
- Put the same filepath into data/city_matrices.py for just one DM solve

# Sampling

- Sampler.py provides utilities for random Sampling of large distance matrices
- Configure filepaths in the code for the naming of specific distance matrix
- If Sample3DMs is used the output is a directory filled with DMs as sub-matrices from original
- Output of Sample3DMs is loaded into data/distance_matrices using Loader.py
- Use BatchQueue.py createRunningFile() to create runFile for this dir
- change main method in BatchQueue.py to runRunningFile(filename) and start through POST Request on webserver
- copy BatchCustom or BatchDir after finished run for the results

# Swapping

- Swap specific IDs of 2 distance matrices by using the Swapper.py class
- Mostly done to create hybrid distance matrices from a real road network DM and a euclidean DM
- Can be used as Swapper.findSwapIds() to swap out a % of biggest difference points

# Other Utility classes

- Finder.py -- Finding the biggest absolute difference between IDs of 2 distance matrices
- Loader.py -- used to load the generated directories from Sampler.py into data/distance_matrices and saves PartnerIDs in a seperate dir
- rebuild_res.py -- used to rebuild pyvrp.Result objects from the json resDicts generated by solves using this framework || can write short .txt summaries of solutions
- R_Project/ -- R code to generate graphs for csv files generated from arcpy shapefile results
- Grapher.py -- generate graphs manually by providing pyvrp.Result objects (only of statistics are collected from the solve)
- circtuiy.py -- introductory implementation to multiply all IDs of a DM by a given circuity factor file --> Needs to be adapted to only multiply certain results for sensible outputs

# Warning

- including or generating too many different distance matrices severly slows down docker build speeds and also uses a lot of disk space to cache build process
- limit data/distance_matrices directory to a max 5-10gb by deleting unused distance_matrices
- hardcoded paths in this code are common --> Changing filenames or paths can be problematic
- c4a instaces are of ARM architecture --> If building on a x86 system use another VM type
- huge manual runs with multiple processes collect a lot of statistics --> results json files grow lineraly with total amount of iterations performed -- It might be sensible to turn of statistics in the code for manual runs aswell if running 16+ solves with over 10k-20k iterations (default for Batch solves using BatchQueue.py)




